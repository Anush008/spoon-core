{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build Custom Tools in SpoonAI\n",
    "\n",
    "This notebook demonstrates how to create custom tools for SpoonAI agents. Tools are fundamental components that allow agents to interact with external systems, process data, and perform specialized tasks.\n",
    "By the end of this tutorial, you will understand:\n",
    "- The basic structure of a SpoonAI tool\n",
    "- How to implement different types of tools\n",
    "- How to integrate tools with an agent\n",
    "- How to test and use your custom tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangzhihao/miniconda3/envs/backend/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from pydantic import Field\n",
    "\n",
    "from spoon_ai.agents import ToolCallAgent\n",
    "from spoon_ai.chat import ChatBot\n",
    "from spoon_ai.tools import ToolManager\n",
    "from spoon_ai.tools.base import BaseTool, ToolFailure, ToolResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Basic Tool Structure\n",
    "In SpoonAI, tools are built by inheriting from the BaseTool class. Each tool needs:\n",
    "1. name: A unique identifier for the tool\n",
    "2. description: A clear description of what the tool does\n",
    "3. parameters: JSON schema defining the input parameters\n",
    "4. execute(): The main method that implements the tool's functionality\n",
    "\n",
    "The parameters schema follows JSON Schema format, which helps the agent understand what inputs are required and their expected types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File System Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileSystemTool(BaseTool):\n",
    "    \"\"\"Tool for interacting with the file system\"\"\"\n",
    "    name: str = \"file_system\"\n",
    "    description: str = \"Perform operations on the file system, such as reading, writing, and listing files.\"\n",
    "    parameters: dict = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The operation to perform (read, write, list)\",\n",
    "                \"enum\": [\"read\", \"write\", \"list\"]\n",
    "            },\n",
    "            \"path\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The file or directory path\"\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The content to write (only for write operation)\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"operation\", \"path\"]\n",
    "    }\n",
    "\n",
    "    async def execute(self, operation: str, path: str, content: Optional[str] = None) -> str:\n",
    "        \"\"\"Execute file system operations\"\"\"\n",
    "        try:\n",
    "            path_obj = Path(path)\n",
    "            \n",
    "            if operation == \"read\":\n",
    "                if not path_obj.exists():\n",
    "                    return f\"Error: File {path} does not exist\"\n",
    "                if not path_obj.is_file():\n",
    "                    return f\"Error: {path} is not a file\"\n",
    "                \n",
    "                with open(path, 'r') as file:\n",
    "                    file_content = file.read()\n",
    "                return f\"Content of {path}:\\n\\n{file_content}\"\n",
    "                \n",
    "            elif operation == \"write\":\n",
    "                if content is None:\n",
    "                    return \"Error: Content parameter is required for write operation\"\n",
    "                \n",
    "                # Create parent directories if they don't exist\n",
    "                path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                with open(path, 'w') as file:\n",
    "                    file.write(content)\n",
    "                return f\"Successfully wrote {len(content)} characters to {path}\"\n",
    "                \n",
    "            elif operation == \"list\":\n",
    "                if not path_obj.exists():\n",
    "                    return f\"Error: Path {path} does not exist\"\n",
    "                if not path_obj.is_dir():\n",
    "                    return f\"Error: {path} is not a directory\"\n",
    "                \n",
    "                files = [f.name for f in path_obj.iterdir()]\n",
    "                return f\"Contents of directory {path}:\\n- \" + \"\\n- \".join(files)\n",
    "                \n",
    "            else:\n",
    "                return f\"Error: Unsupported operation '{operation}'\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error performing {operation} on {path}: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Request Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIRequestTool(BaseTool):\n",
    "    \"\"\"Tool for making HTTP requests to external APIs\"\"\"\n",
    "    name: str = \"api_request\"\n",
    "    description: str = \"Make HTTP requests to external APIs to fetch or send data.\"\n",
    "    parameters: dict = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"method\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"HTTP method (GET, POST, PUT, DELETE)\",\n",
    "                \"enum\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"]\n",
    "            },\n",
    "            \"url\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The URL to make the request to\"\n",
    "            },\n",
    "            \"headers\": {\n",
    "                \"type\": \"object\",\n",
    "                \"description\": \"HTTP headers to include in the request\"\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"type\": \"object\",\n",
    "                \"description\": \"Query parameters for the request\"\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"type\": \"object\",\n",
    "                \"description\": \"Data to send in the request body (for POST/PUT)\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"method\", \"url\"]\n",
    "    }\n",
    "\n",
    "    async def execute(\n",
    "        self, \n",
    "        method: str, \n",
    "        url: str, \n",
    "        headers: Optional[Dict] = None, \n",
    "        params: Optional[Dict] = None, \n",
    "        data: Optional[Dict] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Execute HTTP request\"\"\"\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                headers = headers or {}\n",
    "                params = params or {}\n",
    "                \n",
    "                if method == \"GET\":\n",
    "                    async with session.get(url, headers=headers, params=params) as response:\n",
    "                        response_text = await response.text()\n",
    "                        return self._format_response(response, response_text)\n",
    "                        \n",
    "                elif method == \"POST\":\n",
    "                    async with session.post(url, headers=headers, params=params, json=data) as response:\n",
    "                        response_text = await response.text()\n",
    "                        return self._format_response(response, response_text)\n",
    "                        \n",
    "                elif method == \"PUT\":\n",
    "                    async with session.put(url, headers=headers, params=params, json=data) as response:\n",
    "                        response_text = await response.text()\n",
    "                        return self._format_response(response, response_text)\n",
    "                        \n",
    "                elif method == \"DELETE\":\n",
    "                    async with session.delete(url, headers=headers, params=params) as response:\n",
    "                        response_text = await response.text()\n",
    "                        return self._format_response(response, response_text)\n",
    "                        \n",
    "                else:\n",
    "                    return f\"Error: Unsupported HTTP method '{method}'\"\n",
    "                    \n",
    "        except Exception as e:\n",
    "            return f\"Error making {method} request to {url}: {str(e)}\"\n",
    "    \n",
    "    def _format_response(self, response, text):\n",
    "        \"\"\"Format the HTTP response\"\"\"\n",
    "        status = response.status\n",
    "        try:\n",
    "            # Try to parse as JSON for prettier output\n",
    "            json_data = json.loads(text)\n",
    "            formatted_data = json.dumps(json_data, indent=2)\n",
    "            return f\"Status: {status}\\nResponse:\\n{formatted_data}\"\n",
    "        except:\n",
    "            # If not JSON, return as is\n",
    "            return f\"Status: {status}\\nResponse:\\n{text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Database Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseTool(BaseTool):\n",
    "    \"\"\"Tool for interacting with SQLite databases\"\"\"\n",
    "    name: str = \"database\"\n",
    "    description: str = \"Execute SQL queries on a SQLite database.\"\n",
    "    parameters: dict = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The operation to perform (query, execute)\",\n",
    "                \"enum\": [\"query\", \"execute\"]\n",
    "            },\n",
    "            \"database_path\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Path to the SQLite database file\"\n",
    "            },\n",
    "            \"sql\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"SQL statement to execute\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"operation\", \"database_path\", \"sql\"]\n",
    "    }\n",
    "\n",
    "    async def execute(self, operation: str, database_path: str, sql: str) -> str:\n",
    "        \"\"\"Execute database operations\"\"\"\n",
    "        try:\n",
    "            # Ensure the database directory exists\n",
    "            db_path = Path(database_path)\n",
    "            db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Connect to the database\n",
    "            conn = sqlite3.connect(database_path)\n",
    "            \n",
    "            if operation == \"query\":\n",
    "                # For SELECT queries, return the results as a formatted table\n",
    "                df = pd.read_sql_query(sql, conn)\n",
    "                if df.empty:\n",
    "                    result = \"Query returned no results.\"\n",
    "                else:\n",
    "                    result = f\"Query results:\\n{df.to_string(index=False)}\"\n",
    "                \n",
    "            elif operation == \"execute\":\n",
    "                # For other SQL statements (INSERT, UPDATE, DELETE, CREATE, etc.)\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(sql)\n",
    "                conn.commit()\n",
    "                affected_rows = cursor.rowcount\n",
    "                result = f\"SQL executed successfully. Affected rows: {affected_rows}\"\n",
    "                \n",
    "            else:\n",
    "                result = f\"Error: Unsupported operation '{operation}'\"\n",
    "                \n",
    "            conn.close()\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error executing {operation} on {database_path}: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnalysisTool(BaseTool):\n",
    "    \"\"\"Tool for analyzing data files\"\"\"\n",
    "    name: str = \"data_analysis\"\n",
    "    description: str = \"Analyze data files (CSV, Excel) and perform basic statistical operations.\"\n",
    "    parameters: dict = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"file_path\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Path to the data file (CSV, Excel)\"\n",
    "            },\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Analysis operation to perform\",\n",
    "                \"enum\": [\"summary\", \"head\", \"tail\", \"describe\", \"columns\", \"count\"]\n",
    "            },\n",
    "            \"column\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Column name for column-specific operations\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"file_path\", \"operation\"]\n",
    "    }\n",
    "\n",
    "    async def execute(\n",
    "        self, \n",
    "        file_path: str, \n",
    "        operation: str, \n",
    "        column: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Execute data analysis operations\"\"\"\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(file_path):\n",
    "                return f\"Error: File {file_path} does not exist\"\n",
    "                \n",
    "            # Load the data file\n",
    "            file_ext = os.path.splitext(file_path)[1].lower()\n",
    "            if file_ext == '.csv':\n",
    "                df = pd.read_csv(file_path)\n",
    "            elif file_ext in ['.xlsx', '.xls']:\n",
    "                df = pd.read_excel(file_path)\n",
    "            else:\n",
    "                return f\"Error: Unsupported file format '{file_ext}'\"\n",
    "                \n",
    "            # Perform the requested operation\n",
    "            if operation == \"summary\":\n",
    "                return f\"File: {file_path}\\nRows: {len(df)}\\nColumns: {len(df.columns)}\\nColumn names: {', '.join(df.columns)}\"\n",
    "                \n",
    "            elif operation == \"head\":\n",
    "                return f\"First 5 rows of {file_path}:\\n{df.head().to_string()}\"\n",
    "                \n",
    "            elif operation == \"tail\":\n",
    "                return f\"Last 5 rows of {file_path}:\\n{df.tail().to_string()}\"\n",
    "                \n",
    "            elif operation == \"describe\":\n",
    "                if column and column in df.columns:\n",
    "                    return f\"Statistics for column '{column}':\\n{df[column].describe().to_string()}\"\n",
    "                else:\n",
    "                    return f\"Statistical summary of {file_path}:\\n{df.describe().to_string()}\"\n",
    "                    \n",
    "            elif operation == \"columns\":\n",
    "                return f\"Columns in {file_path}:\\n- \" + \"\\n- \".join(df.columns)\n",
    "                \n",
    "            elif operation == \"count\":\n",
    "                if column and column in df.columns:\n",
    "                    value_counts = df[column].value_counts()\n",
    "                    return f\"Value counts for column '{column}':\\n{value_counts.to_string()}\"\n",
    "                else:\n",
    "                    return f\"Error: Column '{column}' not specified or not found\"\n",
    "                    \n",
    "            else:\n",
    "                return f\"Error: Unsupported operation '{operation}'\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error analyzing {file_path}: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom Agent with the advanced tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from spoon_ai.tools import Terminate\n",
    "class DataAnalystAgent(ToolCallAgent):\n",
    "    \"\"\"Data Analyst Agent with advanced tools\"\"\"\n",
    "    name: str = \"data_analyst\"\n",
    "    description: str = \"An agent that can analyze data, interact with databases, make API calls, and manage files\"\n",
    "    \n",
    "    system_prompt: str = \"\"\"You are a data analyst assistant that can help users with various data-related tasks.\n",
    "    You can:\n",
    "    1. Analyze data files (CSV, Excel)\n",
    "    2. Execute SQL queries on databases\n",
    "    3. Make API requests to fetch data\n",
    "    4. Read and write files\n",
    "    \n",
    "    Use the appropriate tool based on the user's request. Be thorough in your analysis and explanations.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_steps: int = 8\n",
    "    \n",
    "    # Define available tools\n",
    "    avaliable_tools: ToolManager = Field(default_factory=lambda: ToolManager([\n",
    "        FileSystemTool(),\n",
    "        APIRequestTool(),\n",
    "        DatabaseTool(),\n",
    "        DataAnalysisTool(),\n",
    "        Terminate(),\n",
    "    ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data files created.\n",
      "\n",
      "=== Query: Analyze the employees.csv file and give me a summary ===\n",
      "Response:\n",
      "Step 1: Observed output of cmd file_system execution: Contents of directory .:\n",
      "- 0_how_to_build_a_chat_bot.ipynb\n",
      "- examples\n",
      "- 3_how_to_use_neofs_storage.ipynb\n",
      "- 1_how_to_build_a_tool_call_agent.ipynb\n",
      "- 2_how_to_execute_token_transfer_agent.ipynb\n",
      "Step 2: Observed output of cmd file_system execution: Contents of directory examples:\n",
      "- data\n",
      "Step 3: Observed output of cmd file_system execution: Contents of directory examples/data:\n",
      "- employees.csv\n",
      "Step 4: Observed output of cmd data_analysis execution: File: examples/data/employees.csv\n",
      "Rows: 10\n",
      "Columns: 5\n",
      "Column names: id, name, age, city, salary\n",
      "Step 5: I've analyzed the employees.csv file. Here's a summary:\n",
      "\n",
      "**File Information:**\n",
      "- File path: examples/data/employees.csv\n",
      "- Total rows: 10\n",
      "- Total columns: 5\n",
      "\n",
      "**Column Structure:**\n",
      "The file contains the following columns:\n",
      "1. id\n",
      "2. name\n",
      "3. age\n",
      "4. city\n",
      "5. salary\n",
      "\n",
      "Would you like me to provide more detailed information about this dataset, such as descriptive statistics, the first few rows, or specific analysis on any of the columns?\n",
      "Step 6: Observed output of cmd data_analysis execution: Statistical summary of examples/data/employees.csv:\n",
      "             id      age        salary\n",
      "count  10.00000  10.0000     10.000000\n",
      "mean    5.50000  35.5000  83700.000000\n",
      "std     3.02765   6.7536   7746.683878\n",
      "min     1.00000  27.0000  72000.000000\n",
      "25%     3.25000  29.5000  78250.000000\n",
      "50%     5.50000  35.0000  83500.000000\n",
      "75%     7.75000  41.2500  90250.000000\n",
      "max    10.00000  45.0000  95000.000000\n",
      "Step 7: I've performed a more detailed analysis of the employees.csv file. Here's a comprehensive summary:\n",
      "\n",
      "**File Information:**\n",
      "- File path: examples/data/employees.csv\n",
      "- Total rows: 10\n",
      "- Total columns: 5 (id, name, age, city, salary)\n",
      "\n",
      "**Statistical Summary:**\n",
      "\n",
      "For numeric columns:\n",
      "\n",
      "| Statistic | id | age | salary |\n",
      "|-----------|-----|-----|--------|\n",
      "| count | 10 | 10 | 10 |\n",
      "| mean | 5.5 | 35.5 | $83,700 |\n",
      "| std | 3.03 | 6.75 | $7,746.68 |\n",
      "| min | 1 | 27 | $72,000 |\n",
      "| 25% | 3.25 | 29.5 | $78,250 |\n",
      "| 50% (median) | 5.5 | 35 | $83,500 |\n",
      "| 75% | 7.75 | 41.25 | $90,250 |\n",
      "| max | 10 | 45 | $95,000 |\n",
      "\n",
      "**Key Insights:**\n",
      "- The employees range in age from 27 to 45 years, with an average age of 35.5 years.\n",
      "- Salaries range from $72,000 to $95,000, with an average salary of $83,700.\n",
      "- The salary distribution shows a standard deviation of approximately $7,747, indicating moderate variation in compensation.\n",
      "- The median salary ($83,500) is close to the mean, suggesting a relatively symmetric salary distribution.\n",
      "\n",
      "Would you like me to show the first few rows of the dataset or perform any specific analysis on particular columns?\n",
      "Step 8: Observed output of cmd data_analysis execution: First 5 rows of examples/data/employees.csv:\n",
      "   id            name  age           city  salary\n",
      "0   1      John Smith   34       New York   75000\n",
      "1   2    Mary Johnson   28  San Francisco   85000\n",
      "2   3    Robert Brown   45        Chicago   92000\n",
      "3   4  Patricia Davis   31         Boston   78000\n",
      "4   5    James Wilson   39        Seattle   88000\n",
      "Step 8: Stuck in loop. Resetting state.\n",
      "\n",
      "=== Query: What are the average salaries in the employees database? ===\n",
      "Response:\n",
      "Step 1: Observed output of cmd file_system execution: Contents of directory .:\n",
      "- 0_how_to_build_a_chat_bot.ipynb\n",
      "- examples\n",
      "- 3_how_to_use_neofs_storage.ipynb\n",
      "- 1_how_to_build_a_tool_call_agent.ipynb\n",
      "- 2_how_to_execute_token_transfer_agent.ipynb\n",
      "Step 2: I don't see an employees database in the current directory. To help you find the average salaries in an employees database, I'll need more information:\n",
      "\n",
      "1. Is there a specific database file (like a .db or .sqlite file) that contains employee data?\n",
      "2. Or is there perhaps a CSV or Excel file with employee information?\n",
      "3. Do you know the path to the database or file?\n",
      "\n",
      "Once you provide this information, I can help you analyze the average salaries in the employees database.\n",
      "Step 3: Observed output of cmd file_system execution: Contents of directory examples:\n",
      "- data\n",
      "Step 4: Observed output of cmd file_system execution: Contents of directory examples/data:\n",
      "- employees.csv\n",
      "Step 5: Observed output of cmd data_analysis execution: Columns in examples/data/employees.csv:\n",
      "- id\n",
      "- name\n",
      "- age\n",
      "- city\n",
      "- salary\n",
      "Step 6: Observed output of cmd data_analysis execution: Statistics for column 'salary':\n",
      "count       10.000000\n",
      "mean     83700.000000\n",
      "std       7746.683878\n",
      "min      72000.000000\n",
      "25%      78250.000000\n",
      "50%      83500.000000\n",
      "75%      90250.000000\n",
      "max      95000.000000\n",
      "Step 7: # Average Salaries Analysis\n",
      "\n",
      "Based on the analysis of the employees.csv file, here are the salary statistics:\n",
      "\n",
      "## Average Salary\n",
      "The average (mean) salary in the employees database is **$83,700**.\n",
      "\n",
      "## Additional Salary Statistics:\n",
      "- **Minimum salary**: $72,000\n",
      "- **Maximum salary**: $95,000\n",
      "- **Median salary**: $83,500\n",
      "- **Standard deviation**: $7,746.68\n",
      "\n",
      "The data shows that salaries range from $72,000 to $95,000, with 50% of employees earning between $78,250 and $90,250 (the 25th and 75th percentiles).\n",
      "\n",
      "The median salary ($83,500) is close to the mean, suggesting that the salary distribution is relatively symmetric without extreme outliers.\n",
      "Step 8: Observed output of cmd data_analysis execution: First 5 rows of examples/data/employees.csv:\n",
      "   id            name  age           city  salary\n",
      "0   1      John Smith   34       New York   75000\n",
      "1   2    Mary Johnson   28  San Francisco   85000\n",
      "2   3    Robert Brown   45        Chicago   92000\n",
      "3   4  Patricia Davis   31         Boston   78000\n",
      "4   5    James Wilson   39        Seattle   88000\n",
      "Step 8: Stuck in loop. Resetting state.\n",
      "\n",
      "=== Query: Read the first 5 rows of the employees.csv file ===\n",
      "Response:\n",
      "Step 1: Observed output of cmd data_analysis execution: Error: File employees.csv does not exist\n",
      "Step 2: Observed output of cmd file_system execution: Contents of directory .:\n",
      "- 0_how_to_build_a_chat_bot.ipynb\n",
      "- examples\n",
      "- 3_how_to_use_neofs_storage.ipynb\n",
      "- 1_how_to_build_a_tool_call_agent.ipynb\n",
      "- 2_how_to_execute_token_transfer_agent.ipynb\n",
      "Step 3: Observed output of cmd file_system execution: Contents of directory examples:\n",
      "- data\n",
      "Step 4: Observed output of cmd file_system execution: Contents of directory examples/data:\n",
      "- employees.csv\n",
      "Step 5: Observed output of cmd data_analysis execution: First 5 rows of examples/data/employees.csv:\n",
      "   id            name  age           city  salary\n",
      "0   1      John Smith   34       New York   75000\n",
      "1   2    Mary Johnson   28  San Francisco   85000\n",
      "2   3    Robert Brown   45        Chicago   92000\n",
      "3   4  Patricia Davis   31         Boston   78000\n",
      "4   5    James Wilson   39        Seattle   88000\n",
      "Step 6: Here are the first 5 rows of the employees.csv file:\n",
      "\n",
      "| id | name | age | city | salary |\n",
      "|----|------|-----|------|--------|\n",
      "| 1 | John Smith | 34 | New York | 75000 |\n",
      "| 2 | Mary Johnson | 28 | San Francisco | 85000 |\n",
      "| 3 | Robert Brown | 45 | Chicago | 92000 |\n",
      "| 4 | Patricia Davis | 31 | Boston | 78000 |\n",
      "| 5 | James Wilson | 39 | Seattle | 88000 |\n",
      "\n",
      "The file contains employee information with columns for ID, name, age, city, and salary.\n",
      "Step 7: Is there anything else you'd like to know about the employees.csv file or any other data analysis you'd like me to perform? I can help with:\n",
      "\n",
      "1. Getting more detailed statistics about the data\n",
      "2. Analyzing specific columns\n",
      "3. Counting records\n",
      "4. Viewing different portions of the file\n",
      "5. Any other data analysis tasks\n",
      "\n",
      "Just let me know what you'd like to explore next!\n",
      "Step 8: Error: Tool terminate not found\n",
      "Step 8: Stuck in loop. Resetting state.\n",
      "\n",
      "=== Query: How many employees are in each city according to the CSV file? ===\n",
      "Response:\n",
      "Step 1: Observed output of cmd file_system execution: Contents of directory .:\n",
      "- 0_how_to_build_a_chat_bot.ipynb\n",
      "- examples\n",
      "- 3_how_to_use_neofs_storage.ipynb\n",
      "- 1_how_to_build_a_tool_call_agent.ipynb\n",
      "- 2_how_to_execute_token_transfer_agent.ipynb\n",
      "Step 2: I don't see any CSV files in the current directory that would contain employee data. To help you analyze how many employees are in each city, I'll need a CSV file that contains employee information.\n",
      "\n",
      "Could you please:\n",
      "1. Provide the path to the CSV file that contains employee data, or\n",
      "2. Upload a CSV file with employee information?\n",
      "\n",
      "Once I have access to the file, I can analyze it and tell you how many employees are in each city.\n",
      "Step 3: Observed output of cmd file_system execution: Contents of directory examples:\n",
      "- data\n",
      "Step 4: Observed output of cmd file_system execution: Contents of directory examples/data:\n",
      "- employees.csv\n",
      "Step 5: Observed output of cmd data_analysis execution: First 5 rows of examples/data/employees.csv:\n",
      "   id            name  age           city  salary\n",
      "0   1      John Smith   34       New York   75000\n",
      "1   2    Mary Johnson   28  San Francisco   85000\n",
      "2   3    Robert Brown   45        Chicago   92000\n",
      "3   4  Patricia Davis   31         Boston   78000\n",
      "4   5    James Wilson   39        Seattle   88000\n",
      "Step 6: Observed output of cmd data_analysis execution: Columns in examples/data/employees.csv:\n",
      "- id\n",
      "- name\n",
      "- age\n",
      "- city\n",
      "- salary\n",
      "Step 7: Observed output of cmd data_analysis execution: Value counts for column 'city':\n",
      "city\n",
      "New York         1\n",
      "San Francisco    1\n",
      "Chicago          1\n",
      "Boston           1\n",
      "Seattle          1\n",
      "Austin           1\n",
      "Denver           1\n",
      "Portland         1\n",
      "Los Angeles      1\n",
      "Miami            1\n",
      "Step 8: # Employee Count by City\n",
      "\n",
      "Based on the analysis of the employees.csv file, here's the breakdown of employees in each city:\n",
      "\n",
      "| City | Number of Employees |\n",
      "|------|---------------------|\n",
      "| New York | 1 |\n",
      "| San Francisco | 1 |\n",
      "| Chicago | 1 |\n",
      "| Boston | 1 |\n",
      "| Seattle | 1 |\n",
      "| Austin | 1 |\n",
      "| Denver | 1 |\n",
      "| Portland | 1 |\n",
      "| Los Angeles | 1 |\n",
      "| Miami | 1 |\n",
      "\n",
      "The data shows that there is exactly one employee in each of the 10 cities represented in the dataset.\n",
      "Step 8: Stuck in loop. Resetting state.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Run the example\n",
    "async def main():\n",
    "    # Create a sample CSV file for demonstration\n",
    "    sample_data = \"\"\"id,name,age,city,salary\n",
    "1,John Smith,34,New York,75000\n",
    "2,Mary Johnson,28,San Francisco,85000\n",
    "3,Robert Brown,45,Chicago,92000\n",
    "4,Patricia Davis,31,Boston,78000\n",
    "5,James Wilson,39,Seattle,88000\n",
    "6,Jennifer Moore,27,Austin,72000\n",
    "7,Michael Taylor,42,Denver,95000\n",
    "8,Elizabeth Anderson,36,Portland,82000\n",
    "9,David Thomas,29,Los Angeles,79000\n",
    "10,Susan Jackson,44,Miami,91000\n",
    "\"\"\"\n",
    "    \n",
    "    os.makedirs(\"examples/data\", exist_ok=True)\n",
    "    with open(\"examples/data/employees.csv\", \"w\") as f:\n",
    "        f.write(sample_data)\n",
    "    \n",
    "    \n",
    "    print(\"Sample data files created.\")\n",
    "    # Create the agent\n",
    "    # print(os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "    agent = DataAnalystAgent(llm=ChatBot(api_key=os.environ[\"ANTHROPIC_API_KEY\"]))\n",
    "    \n",
    "    # Run the agent with different queries\n",
    "    queries = [\n",
    "        \"Analyze the employees.csv file and give me a summary\",\n",
    "        \"What are the average salaries in the employees database?\",\n",
    "        \"Read the first 5 rows of the employees.csv file\",\n",
    "        \"How many employees are in each city according to the CSV file?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\n=== Query: {query} ===\")\n",
    "        response = await agent.run(query)\n",
    "        print(f\"Response:\\n{response}\")\n",
    "        \n",
    "        # Reset agent state for the next query\n",
    "        agent.clear()\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
